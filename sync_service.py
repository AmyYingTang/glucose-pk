"""
Dexcom æ•°æ®åŒæ­¥æœåŠ¡
åå°çº¿ç¨‹å®šæ—¶ä» Dexcom API æ‹‰å–æ•°æ®ï¼Œå­˜å‚¨åˆ°æœ¬åœ° JSON æ–‡ä»¶

ç‰¹æ€§ï¼š
- æ¯ 3 åˆ†é’ŸåŒæ­¥ä¸€æ¬¡ï¼ˆDexcom æ•°æ®æ¯ 5 åˆ†é’Ÿæ›´æ–°ï¼‰
- æœ¬åœ° JSON å­˜å‚¨ï¼ŒæŒ‰ç”¨æˆ·åˆ†æ–‡ä»¶
- ä¿ç•™ 48 å°æ—¶å†å²æ•°æ®
- çº¿ç¨‹å®‰å…¨çš„è¯»å†™
- è‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®
"""

import os
import json
import time
import threading
from datetime import datetime, timedelta
from pydexcom import Dexcom
from config import USERS

# ==================== é…ç½® ====================

# åŒæ­¥é—´éš”ï¼ˆç§’ï¼‰
SYNC_INTERVAL = 180  # 3 åˆ†é’Ÿ

# æœ¬åœ°æ•°æ®ç›®å½•
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "glucose_data")

# ä¿ç•™å¤šå°‘å°æ—¶çš„å†å²æ•°æ®
HISTORY_HOURS = 48

# ==================== å…¨å±€çŠ¶æ€ ====================

# Dexcom å®¢æˆ·ç«¯ç¼“å­˜
_dexcom_clients = {}
_clients_lock = threading.Lock()

# æ•°æ®è¯»å†™é”ï¼ˆæ¯ä¸ªç”¨æˆ·ä¸€ä¸ªï¼‰
_data_locks = {}

# åŒæ­¥çŠ¶æ€
sync_status = {
    "last_sync": None,
    "last_success": None,
    "errors": [],
    "is_running": False
}


def _get_data_lock(user_id):
    """è·å–ç”¨æˆ·çš„æ•°æ®é”"""
    if user_id not in _data_locks:
        _data_locks[user_id] = threading.Lock()
    return _data_locks[user_id]


def _get_dexcom_client(user_id):
    """è·å– Dexcom å®¢æˆ·ç«¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    global _dexcom_clients
    
    if user_id not in USERS:
        raise ValueError(f"æœªçŸ¥ç”¨æˆ·: {user_id}")
    
    with _clients_lock:
        if user_id not in _dexcom_clients:
            user = USERS[user_id]
            _dexcom_clients[user_id] = Dexcom(
                username=user["username"],
                password=user["password"],
                region=user.get("region", "us")
            )
    
    return _dexcom_clients[user_id]


def _get_user_data_file(user_id):
    """è·å–ç”¨æˆ·æ•°æ®æ–‡ä»¶è·¯å¾„"""
    os.makedirs(DATA_DIR, exist_ok=True)
    return os.path.join(DATA_DIR, f"{user_id}.json")


# ==================== æ•°æ®è¯»å†™ ====================

def load_user_data(user_id):
    """
    ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ç”¨æˆ·æ•°æ®
    
    Returns:
        dict: {
            "user_id": "user1",
            "last_updated": "2025-01-15T14:35:00",
            "current": { ... },
            "history": [ ... ]
        }
    """
    filepath = _get_user_data_file(user_id)
    lock = _get_data_lock(user_id)
    
    with lock:
        if os.path.exists(filepath):
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ {user_id} æ•°æ®å¤±è´¥: {e}")
    
    # è¿”å›ç©ºæ•°æ®ç»“æ„
    return {
        "user_id": user_id,
        "last_updated": None,
        "current": None,
        "history": []
    }


def save_user_data(user_id, data):
    """ä¿å­˜ç”¨æˆ·æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶"""
    filepath = _get_user_data_file(user_id)
    lock = _get_data_lock(user_id)
    
    with lock:
        try:
            # å…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½åï¼ˆåŸå­æ“ä½œï¼‰
            temp_file = filepath + ".tmp"
            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            os.replace(temp_file, filepath)
        except Exception as e:
            print(f"âŒ ä¿å­˜ {user_id} æ•°æ®å¤±è´¥: {e}")


def clean_old_history(history, hours=HISTORY_HOURS):
    """æ¸…ç†è¿‡æœŸçš„å†å²æ•°æ®"""
    if not history:
        return []
    
    cutoff = datetime.now() - timedelta(hours=hours)
    
    cleaned = []
    for item in history:
        try:
            item_time = datetime.fromisoformat(item["datetime"].replace("Z", "+00:00"))
            # è½¬ä¸ºæ— æ—¶åŒºæ¯”è¾ƒ
            if item_time.replace(tzinfo=None) > cutoff:
                cleaned.append(item)
        except:
            continue
    
    return cleaned


# ==================== Dexcom åŒæ­¥ ====================

def sync_user_data(user_id, warmup=False):
    """
    ä» Dexcom åŒæ­¥å•ä¸ªç”¨æˆ·çš„æ•°æ®
    
    Args:
        user_id: ç”¨æˆ· ID
        warmup: æ˜¯å¦ä¸ºé¢„çƒ­æ¨¡å¼ï¼ˆæ‹‰å–æ›´å¤šå†å²æ•°æ®ï¼‰
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        client = _get_dexcom_client(user_id)
        
        # è·å–å½“å‰è¡€ç³–
        current_reading = client.get_current_glucose_reading()
        
        # é¢„çƒ­æ¨¡å¼æ‹‰å– 24 å°æ—¶ï¼Œæ­£å¸¸æ¨¡å¼æ‹‰å– 3 å°æ—¶
        if warmup:
            history_minutes = 1440  # 24 å°æ—¶
            history_count = 288     # 24 å°æ—¶ Ã— 12 ä¸ª/å°æ—¶
        else:
            history_minutes = 180   # 3 å°æ—¶
            history_count = 36
        
        history_readings = client.get_glucose_readings(minutes=history_minutes, max_count=history_count)
        
        # åŠ è½½ç°æœ‰æœ¬åœ°æ•°æ®
        local_data = load_user_data(user_id)
        
        # æ›´æ–°å½“å‰å€¼
        if current_reading:
            local_data["current"] = {
                "value": current_reading.mmol_l,
                "value_mgdl": current_reading.value,
                "trend": current_reading.trend,
                "trend_direction": current_reading.trend_direction,
                "trend_description": current_reading.trend_description,
                "trend_arrow": current_reading.trend_arrow,
                "datetime": current_reading.datetime.isoformat(),
            }
        
        # åˆå¹¶å†å²æ•°æ®ï¼ˆå»é‡ï¼‰
        existing_times = set()
        for item in local_data.get("history", []):
            existing_times.add(item.get("datetime"))
        
        new_history = list(local_data.get("history", []))
        
        for reading in history_readings:
            dt_str = reading.datetime.isoformat()
            if dt_str not in existing_times:
                new_history.append({
                    "value": reading.mmol_l,
                    "value_mgdl": reading.value,
                    "trend_arrow": reading.trend_arrow,
                    "datetime": dt_str,
                })
                existing_times.add(dt_str)
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        new_history.sort(key=lambda x: x["datetime"], reverse=True)
        
        # æ¸…ç†è¿‡æœŸæ•°æ®
        new_history = clean_old_history(new_history)
        
        local_data["history"] = new_history
        local_data["last_updated"] = datetime.now().isoformat()
        
        # ä¿å­˜åˆ°æœ¬åœ°
        save_user_data(user_id, local_data)
        
        return True
        
    except Exception as e:
        print(f"âŒ åŒæ­¥ {user_id} å¤±è´¥: {e}")
        return False


def check_needs_warmup(user_id):
    """
    æ£€æŸ¥ç”¨æˆ·æ˜¯å¦éœ€è¦é¢„çƒ­ï¼ˆæœ¬åœ°æ•°æ®ä¸ºç©ºæˆ–ä¸è¶³ 24 å°æ—¶ï¼‰
    """
    local_data = load_user_data(user_id)
    history = local_data.get("history", [])
    
    if not history:
        return True
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦è¦†ç›– 24 å°æ—¶
    try:
        oldest = min(history, key=lambda x: x["datetime"])
        oldest_time = datetime.fromisoformat(oldest["datetime"].replace("Z", "+00:00"))
        age = datetime.now() - oldest_time.replace(tzinfo=None)
        
        # å¦‚æœæœ€è€çš„æ•°æ®ä¸åˆ° 20 å°æ—¶ï¼Œéœ€è¦é¢„çƒ­
        if age < timedelta(hours=20):
            return True
    except:
        return True
    
    return False


def warmup_all_users():
    """é¢„çƒ­æ‰€æœ‰ç”¨æˆ·æ•°æ®ï¼ˆæ‹‰å– 24 å°æ—¶å†å²ï¼‰"""
    print("ğŸ”¥ æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„çƒ­æ•°æ®...")
    
    for user_id in USERS.keys():
        if check_needs_warmup(user_id):
            print(f"   ğŸ“¥ é¢„çƒ­ {user_id} æ•°æ®ï¼ˆæ‹‰å–24å°æ—¶å†å²ï¼‰...")
            if sync_user_data(user_id, warmup=True):
                local_data = load_user_data(user_id)
                print(f"   âœ… {user_id} é¢„çƒ­å®Œæˆï¼Œå…± {len(local_data.get('history', []))} æ¡æ•°æ®")
            else:
                print(f"   âŒ {user_id} é¢„çƒ­å¤±è´¥")
        else:
            print(f"   âœ“ {user_id} æ•°æ®å……è¶³ï¼Œæ— éœ€é¢„çƒ­")


def sync_all_users():
    """åŒæ­¥æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®"""
    global sync_status
    
    sync_status["last_sync"] = datetime.now().isoformat()
    sync_status["is_running"] = True
    
    success_count = 0
    errors = []
    
    for user_id in USERS.keys():
        try:
            if sync_user_data(user_id, warmup=False):
                success_count += 1
            else:
                errors.append(f"{user_id}: åŒæ­¥å¤±è´¥")
        except Exception as e:
            errors.append(f"{user_id}: {str(e)}")
    
    if success_count == len(USERS):
        sync_status["last_success"] = datetime.now().isoformat()
    
    sync_status["errors"] = errors[-10:]  # åªä¿ç•™æœ€è¿‘ 10 æ¡é”™è¯¯
    sync_status["is_running"] = False
    
    print(f"âœ… æ•°æ®åŒæ­¥å®Œæˆ: {success_count}/{len(USERS)} æˆåŠŸ ({datetime.now().strftime('%H:%M:%S')})")


# ==================== åå°çº¿ç¨‹ ====================

def _sync_loop():
    """åå°åŒæ­¥å¾ªç¯"""
    print(f"ğŸ”„ Dexcom åŒæ­¥æœåŠ¡å¯åŠ¨ï¼Œé—´éš”: {SYNC_INTERVAL}ç§’")
    
    # å¯åŠ¨æ—¶å…ˆé¢„çƒ­ï¼ˆè¡¥è¶³ 24 å°æ—¶æ•°æ®ï¼‰
    warmup_all_users()
    
    # ç„¶åæ­£å¸¸åŒæ­¥ä¸€æ¬¡
    sync_all_users()
    
    while True:
        time.sleep(SYNC_INTERVAL)
        try:
            sync_all_users()
        except Exception as e:
            print(f"âŒ åŒæ­¥å¾ªç¯å¼‚å¸¸: {e}")


_sync_thread = None


def start_sync_service():
    """å¯åŠ¨åå°åŒæ­¥æœåŠ¡"""
    global _sync_thread
    
    if _sync_thread is not None and _sync_thread.is_alive():
        print("âš ï¸ åŒæ­¥æœåŠ¡å·²åœ¨è¿è¡Œ")
        return
    
    _sync_thread = threading.Thread(target=_sync_loop, daemon=True)
    _sync_thread.start()
    print("ğŸš€ åå°åŒæ­¥æœåŠ¡å·²å¯åŠ¨")


def get_sync_status():
    """è·å–åŒæ­¥çŠ¶æ€"""
    return sync_status.copy()


# ==================== ä¾› data_fetcher è°ƒç”¨çš„æ¥å£ ====================

def get_current_from_local(user_id):
    """
    ä»æœ¬åœ°è·å–å½“å‰è¡€ç³–ï¼ˆä¾› data_fetcher è°ƒç”¨ï¼‰
    
    Returns:
        dict: å½“å‰è¡€ç³–æ•°æ®ï¼Œæˆ– None
    """
    data = load_user_data(user_id)
    return data.get("current")


def get_history_from_local(user_id, minutes=180, max_count=36):
    """
    ä»æœ¬åœ°è·å–å†å²æ•°æ®ï¼ˆä¾› data_fetcher è°ƒç”¨ï¼‰
    
    Args:
        user_id: ç”¨æˆ· ID
        minutes: è·å–å¤šå°‘åˆ†é’Ÿå†…çš„æ•°æ®
        max_count: æœ€å¤§æ¡æ•°
    
    Returns:
        list: å†å²æ•°æ®åˆ—è¡¨
    """
    data = load_user_data(user_id)
    history = data.get("history", [])
    
    if not history:
        return []
    
    # è¿‡æ»¤æ—¶é—´èŒƒå›´
    cutoff = datetime.now() - timedelta(minutes=minutes)
    filtered = []
    
    for item in history:
        try:
            item_time = datetime.fromisoformat(item["datetime"].replace("Z", "+00:00"))
            if item_time.replace(tzinfo=None) > cutoff:
                filtered.append(item)
        except:
            continue
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰ï¼Œé™åˆ¶æ¡æ•°
    filtered.sort(key=lambda x: x["datetime"], reverse=True)
    
    return filtered[:max_count]


# ==================== æµ‹è¯•å…¥å£ ====================

if __name__ == "__main__":
    print("æµ‹è¯• Dexcom åŒæ­¥æœåŠ¡...")
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ç”¨æˆ·åˆ—è¡¨: {list(USERS.keys())}")
    
    # åŒæ­¥ä¸€æ¬¡
    sync_all_users()
    
    # æ˜¾ç¤ºç»“æœ
    for user_id in USERS.keys():
        data = load_user_data(user_id)
        print(f"\n{user_id}:")
        print(f"  æœ€åæ›´æ–°: {data.get('last_updated')}")
        if data.get('current'):
            print(f"  å½“å‰è¡€ç³–: {data['current'].get('value')} mmol/L")
        print(f"  å†å²æ¡æ•°: {len(data.get('history', []))}")
